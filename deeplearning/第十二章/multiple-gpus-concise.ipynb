{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada69279",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 多GPU的简洁实现\n",
    ":label:`sec_multi_gpu_concise`\n",
    "\n",
    "每个新模型的并行计算都从零开始实现是无趣的。此外，优化同步工具以获得高性能也是有好处的。下面我们将展示如何使用深度学习框架的高级API来实现这一点。数学和算法与 :numref:`sec_multi_gpu`中的相同。本节的代码至少需要两个GPU来运行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5026c6ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:26:08.580318Z",
     "iopub.status.busy": "2022-12-07T17:26:08.579973Z",
     "iopub.status.idle": "2022-12-07T17:26:12.777861Z",
     "shell.execute_reply": "2022-12-07T17:26:12.777029Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf6d51",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## [**简单网络**]\n",
    "\n",
    "让我们使用一个比 :numref:`sec_multi_gpu`的LeNet更有意义的网络，它依然能够容易地和快速地训练。我们选择的是 :cite:`He.Zhang.Ren.ea.2016`中的ResNet-18。因为输入的图像很小，所以稍微修改了一下。与 :numref:`sec_resnet`的区别在于，我们在开始时使用了更小的卷积核、步长和填充，而且删除了最大汇聚层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3069f5f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:26:12.781879Z",
     "iopub.status.busy": "2022-12-07T17:26:12.781495Z",
     "iopub.status.idle": "2022-12-07T17:26:12.789773Z",
     "shell.execute_reply": "2022-12-07T17:26:12.789047Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def resnet18(num_classes, in_channels=1):\n",
    "    \"\"\"稍加修改的ResNet-18模型\"\"\"\n",
    "    def resnet_block(in_channels, out_channels, num_residuals,\n",
    "                     first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(d2l.Residual(in_channels, out_channels,\n",
    "                                        use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(d2l.Residual(out_channels, out_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "    # 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU())\n",
    "    net.add_module(\"resnet_block1\", resnet_block(\n",
    "        64, 64, 2, first_block=True))\n",
    "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n",
    "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1,1)))\n",
    "    net.add_module(\"fc\", nn.Sequential(nn.Flatten(),\n",
    "                                       nn.Linear(512, num_classes)))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e405725",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## 网络初始化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c58c8",
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "我们将在训练回路中初始化网络。请参见 :numref:`sec_numerical_stability`复习初始化方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2aa3a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:26:12.793086Z",
     "iopub.status.busy": "2022-12-07T17:26:12.792655Z",
     "iopub.status.idle": "2022-12-07T17:26:12.929255Z",
     "shell.execute_reply": "2022-12-07T17:26:12.928405Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "net = resnet18(10)\n",
    "# 获取GPU列表\n",
    "devices = d2l.try_all_gpus()\n",
    "# 我们将在训练代码实现中初始化网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351b9f5",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "## [**训练**]\n",
    "\n",
    "如前所述，用于训练的代码需要执行几个基本功能才能实现高效并行：\n",
    "\n",
    "* 需要在所有设备上初始化网络参数；\n",
    "* 在数据集上迭代时，要将小批量数据分配到所有设备上；\n",
    "* 跨设备并行计算损失及其梯度；\n",
    "* 聚合梯度，并相应地更新参数。\n",
    "\n",
    "最后，并行地计算精确度和发布网络的最终性能。除了需要拆分和聚合数据外，训练代码与前几章的实现非常相似。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d40cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:26:12.933044Z",
     "iopub.status.busy": "2022-12-07T17:26:12.932756Z",
     "iopub.status.idle": "2022-12-07T17:26:12.941244Z",
     "shell.execute_reply": "2022-12-07T17:26:12.940497Z"
    },
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def train(net, num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    def init_weights(m):\n",
    "        if type(m) in [nn.Linear, nn.Conv2d]:\n",
    "            nn.init.normal_(m.weight, std=0.01)\n",
    "    net.apply(init_weights)\n",
    "    # 在多个GPU上设置模型\n",
    "    net = nn.DataParallel(net, device_ids=devices)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_epochs = d2l.Timer(), 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(devices[0]), y.to(devices[0])\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "        timer.stop()\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\n",
    "    print(f'测试精度：{animator.Y[0][-1]:.2f}，{timer.avg():.1f}秒/轮，'\n",
    "          f'在{str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fca453",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "接下来看看这在实践中是如何运作的。我们先[**在单个GPU上训练网络**]进行预热。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cc67b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:26:12.944327Z",
     "iopub.status.busy": "2022-12-07T17:26:12.944049Z",
     "iopub.status.idle": "2022-12-07T17:28:44.400307Z",
     "shell.execute_reply": "2022-12-07T17:28:44.399425Z"
    },
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "train(net, num_gpus=1, batch_size=256, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974469c",
   "metadata": {
    "origin_pos": 27
   },
   "source": [
    "接下来我们[**使用2个GPU进行训练**]。与 :numref:`sec_multi_gpu`中评估的LeNet相比，ResNet-18的模型要复杂得多。这就是显示并行化优势的地方，计算所需时间明显大于同步参数需要的时间。因为并行化开销的相关性较小，因此这种操作提高了模型的可伸缩性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682094df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T17:28:44.404401Z",
     "iopub.status.busy": "2022-12-07T17:28:44.403743Z",
     "iopub.status.idle": "2022-12-07T17:30:16.824087Z",
     "shell.execute_reply": "2022-12-07T17:30:16.823186Z"
    },
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "train(net, num_gpus=2, batch_size=512, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcbf4e7",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## 小结\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b516bf",
   "metadata": {
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "* 神经网络可以在（可找到数据的）单GPU上进行自动评估。\n",
    "* 每台设备上的网络需要先初始化，然后再尝试访问该设备上的参数，否则会遇到错误。\n",
    "* 优化算法在多个GPU上自动聚合。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee07ce",
   "metadata": {},
   "source": [
    "使用nn.DataParallel将模型并行化，能够充分利用多个GPU的计算资源，提高训练效率。\n",
    "通过跨设备计算梯度并聚合，优化算法在多个GPU上能够自动同步，确保模型的正确更新。\n",
    "通过调整批量大小和学习率，可以优化训练过程的性能。\n",
    "这种方式简化了多GPU训练的实现，通过PyTorch的高级API，开发者可以更容易地实现并行化计算，并有效利用硬件资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b776cb",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "## 练习\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de9a6e",
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "1. 本节使用ResNet-18，请尝试不同的迭代周期数、批量大小和学习率，以及使用更多的GPU进行计算。如果使用$16$个GPU（例如，在AWS p2.16xlarge实例上）尝试此操作，会发生什么？\n",
    "1. 有时候不同的设备提供了不同的计算能力，我们可以同时使用GPU和CPU，那应该如何分配工作？为什么？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c9866",
   "metadata": {
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/2803)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
